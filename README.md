# ğŸ¤ğŸ¤– Multilingual Voice Chatbot

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Flask](https://img.shields.io/badge/Flask-2.0+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![AI](https://img.shields.io/badge/AI-Powered-red.svg)

**A real-time multilingual voice chatbot with speech-to-speech capabilities**

*ASR â†’ LLM â†’ TTS Pipeline supporting Tamil, Hindi, Telugu, Kannada, and English*

</div>

---

## ğŸŒŸ Features

- ğŸ¤ **Voice Input**: Speech-to-text using OpenAI Whisper
- ğŸ§  **Intelligent Responses**: Powered by Google Gemini 2.5 Pro LLM
- ğŸ” **Real-time Search**: Google Search integration for current information
- ğŸ—£ï¸ **Voice Output**: Text-to-speech in multiple languages using Facebook MMS-TTS
- ğŸŒ **Multilingual Support**: Tamil, Hindi, Telugu, Kannada, and English
- ğŸ•¸ï¸ **Web API**: RESTful Flask API for easy integration
- ğŸ¤– **ReAct Agent**: Intelligent reasoning and action framework

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Demo](#-demo)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage](#-usage)
- [API Reference](#-api-reference)
- [Project Structure](#-project-structure)
- [Technologies Used](#-technologies-used)
- [Contributing](#-contributing)
- [License](#-license)
- [Author](#-author)

## ğŸ¬ Demo

The chatbot can handle questions like:
- "What's the weather today?" (English)
- "à¤«à¥à¤°à¤¾à¤‚à¤¸ à¤•à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?" (Hindi)
- "à°ˆ à°°à±‹à°œà± à°µà°¾à°°à±à°¤à°²à± à°à°®à°¿à°Ÿà°¿?" (Telugu)
- "à²‡à²‚à²¦à²¿à²¨ à²¸à³à²¦à³à²¦à²¿ à²à²¨à³?" (Kannada)
- "à®‡à®©à¯à®±à¯ˆà®¯ à®šà¯†à®¯à¯à®¤à®¿à®•à®³à¯ à®à®©à¯à®©?" (Tamil)

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- FFmpeg (required by Whisper)

### Install FFmpeg

```bash
# macOS
brew install ffmpeg

# Ubuntu/Debian
sudo apt update
sudo apt install ffmpeg

# Windows
# Download from https://ffmpeg.org/download.html
```

### Clone the Repository

```bash
git clone https://github.com/santykamal2001/multilingual-voice-chatbot.git
cd multilingual-voice-chatbot
```

### Create Virtual Environment

```bash
python -m venv voice_chatbot_env
source voice_chatbot_env/bin/activate  # On Windows: voice_chatbot_env\Scripts\activate
```

### Install Dependencies

```bash
pip install -r requirements.txt
```

## âš™ï¸ Configuration

### 1. Get API Keys

#### Google Gemini API Key
1. Go to [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Create a new API key
3. Copy the API key

#### Google Custom Search Engine
1. Go to [Google Custom Search Engine](https://cse.google.com/cse/)
2. Create a new search engine
3. Get your Search Engine ID (CSE ID)
4. Enable the Custom Search API in [Google Cloud Console](https://console.cloud.google.com/)

### 2. Environment Variables

Create a `.env` file in the project root:

```env
GEMINI_API_KEY=your_gemini_api_key_here
GOOGLE_CSE_ID=your_google_custom_search_engine_id_here
```

âš ï¸ **Never commit your `.env` file to version control!**

## ğŸƒ Usage

### Start the Flask Server

```bash
# Make sure your virtual environment is activated
source voice_chatbot_env/bin/activate  # On Windows: voice_chatbot_env\Scripts\activate

# Set the Flask app
export FLASK_APP=app.py  # On Windows: set FLASK_APP=app.py

# Run the server
flask run
```

The server will start at `http://127.0.0.1:5000`

### Using the API

Send a POST request to `/ask` endpoint:

```python
import requests
import base64

# Text input example
response = requests.post('http://127.0.0.1:5000/ask', 
    json={'text_input': 'What is the capital of France?'})
print(response.json())

# Audio input example (with base64 encoded audio)
with open('audio.wav', 'rb') as audio_file:
    audio_b64 = base64.b64encode(audio_file.read()).decode('utf-8')

response = requests.post('http://127.0.0.1:5000/ask',
    json={'audio_input': audio_b64})
print(response.json())
```

## ğŸ“¡ API Reference

### POST `/ask`

Send text or audio input to get AI response with audio output.

#### Request Body

```json
{
  "text_input": "Your question here",
  "audio_input": "base64_encoded_audio"
}
```

*Note: Provide either `text_input` OR `audio_input`, not both*

#### Response

```json
{
  "answer": "AI response text",
  "audio_b64": "base64_encoded_response_audio",
  "sampling_rate": 16000,
  "language_code": "en",
  "language_name": "English"
}
```

#### Error Response

```json
{
  "error": "Error description"
}
```

## ğŸ“ Project Structure

```
multilingual-voice-chatbot/
â”œâ”€â”€ app.py                 # Main Flask application
â”œâ”€â”€ chatbot.py            # Core chatbot implementation
â”œâ”€â”€ asr_test.py           # ASR testing utilities
â”œâ”€â”€ rag_setup.py          # RAG (Retrieval Augmented Generation) setup
â”œâ”€â”€ demo_chatbot.py       # Demo/testing script
â”œâ”€â”€ knowledge_base.txt    # Multilingual knowledge base
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env                  # Environment variables (create this)
â”œâ”€â”€ .gitignore           # Git ignore rules
â”œâ”€â”€ README.md            # This file
â”œâ”€â”€ chroma_db/           # Vector database (auto-created)
â””â”€â”€ voice_chatbot_env/   # Virtual environment (auto-created)
```

## ğŸ› ï¸ Technologies Used

| Component | Technology | Purpose |
|-----------|------------|---------|
| **ASR** | OpenAI Whisper | Convert speech to text |
| **LLM** | Google Gemini 2.5 Pro | Generate intelligent responses |
| **TTS** | Facebook MMS-TTS | Convert text to speech |
| **Search** | Google Custom Search API | Real-time information retrieval |
| **Framework** | LangChain | LLM orchestration and agents |
| **Web API** | Flask + Flask-CORS | RESTful API server |
| **Audio** | soundfile, scipy | Audio processing |
| **ML** | PyTorch, Transformers | Model inference |
| **Language** | langdetect, langcodes | Language detection and handling |

## ğŸ”§ Troubleshooting

### Common Issues

1. **ModuleNotFoundError**: Make sure virtual environment is activated and dependencies are installed
2. **FFmpeg not found**: Install FFmpeg using the instructions above
3. **API Key errors**: Check your `.env` file and ensure API keys are correct
4. **Audio issues**: Ensure audio files are in WAV format for best compatibility
5. **Memory issues**: The models require significant RAM; consider using smaller Whisper models

### Model Loading Times

- First run will download models (may take 5-10 minutes)
- Whisper model: ~150MB
- TTS models: ~50MB each (downloaded on demand)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¨â€ğŸ’» Author

**Santhos Kamal Arumugam Balamurugan**
- ğŸ“ MEng in Computer Science @ Oregon State University
- ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/santhos-kamal-arumugam-balamurugan-6915b41ba/)
- ğŸ™ [GitHub](https://github.com/santykamal2001)

---

<div align="center">

**If you found this project helpful, please give it a â­!**

</div>
